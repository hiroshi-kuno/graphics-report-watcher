{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c03d90",
   "metadata": {},
   "source": [
    "---\n",
    "## watchlistのcsvをjsonに変換する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6acdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wachlist_json():\n",
    "    df= pd.read_csv('watchlist/watchlist.csv')\n",
    "\n",
    "    # jsonファイルに変換してファイルに保存\n",
    "    json_data = df.to_json(orient='records', force_ascii=False)\n",
    "    with open (\"watchlist/watchlist.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5f023",
   "metadata": {},
   "source": [
    "---\n",
    "## 期待通りスクレイピングできるか試す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fb66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_head(url, selector):\n",
    "    response_test =requests.get(url)\n",
    "    soup_test = BeautifulSoup(response_test.content, 'html.parser')\n",
    "    headline_test = soup_test.select(selector)\n",
    "    print(headline_test)\n",
    "\n",
    "# test_get_head(\n",
    "#     \"https://www3.nhk.or.jp/news/catnew.html\",\n",
    "#     \"#main > article > section > div > ul > li:nth-child(1) > dl > dd > a\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eaf338",
   "metadata": {},
   "source": [
    "---\n",
    "## watchlistの一覧を元に見出しを見に行く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07eba9be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日経電子版 速報\n",
      "New title\n",
      "楽天、安楽智大を自宅待機に　ハラスメント疑い\n",
      "Before title\n",
      "楽天、安楽智大を自宅待機に　ハラスメント疑い\n",
      "No update\n",
      "\n",
      "NHKニュース 速報・新着一覧\n",
      "New title\n",
      "フィギュア NHK杯 鍵山優真3年ぶり優勝 宇野昌磨2位【全結果】\n",
      "Before title\n",
      "【随時更新】ハマス 人質24人を解放 戦闘休止後初\n",
      "Update\n",
      "\n",
      "Yahoo!ニュース 速報\n",
      "New title\n",
      "＜第15回TAMA映画賞＞最優秀作品賞は「怪物」「雑魚どもよ、大志を抱け！」　旬の俳優が授賞式に\n",
      "Before title\n",
      "＜第15回TAMA映画賞＞最優秀作品賞は「怪物」「雑魚どもよ、大志を抱け！」　旬の俳優が授賞式に\n",
      "No update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 現状の一覧を開く（リセットする時はwatchlistをひらく）\n",
    "# with open('./watchlist/watchlist.json', 'r') as json_file:\n",
    "with open('./get_headers.json', 'r') as json_file:\n",
    "    websites = json.load(json_file)\n",
    "\n",
    "# サイト毎に見出しを見に行き、前回との差分を見る\n",
    "for site in websites:\n",
    "    response =requests.get(site[\"url\"])\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headlines = soup.select(site[\"selector\"])\n",
    "    \n",
    "    # 前回の見出しを保存しておく\n",
    "    site[\"get_title_before\"] = site[\"get_title\"]\n",
    "    # 取得した見出しを保存\n",
    "    site[\"get_title\"] = headlines[0].text.strip()\n",
    "\n",
    "    # 結果を表示\n",
    "    print(\n",
    "        site[\"head\"] +\n",
    "        \"\\nNew title\\n\" +\n",
    "        str(site[\"get_title\"]) + \n",
    "        \"\\nBefore title\\n\" +\n",
    "        str(site[\"get_title_before\"])\n",
    "    )\n",
    "\n",
    "    # 差分のが出たかを見て、値として保存\n",
    "    if site[\"get_title_before\"] != site[\"get_title\"]:\n",
    "        print(\"Update\\n\")\n",
    "        site[\"update\"] = True\n",
    "    else:\n",
    "        print(\"No update\\n\")        \n",
    "        site[\"update\"] = False\n",
    "\n",
    "# 処理の結果を保存\n",
    "with open(\"./get_headers.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(websites, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b02b6",
   "metadata": {},
   "source": [
    "---\n",
    "## アップデートがあったものだけ抽出して、jsonの配列に追加して保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42b1412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'date': '2023/11/25 22:47',\n",
       " 'updates': [{'head': 'NHKニュース 速報・新着一覧',\n",
       "   'media': 'NHK',\n",
       "   'url': 'https://www3.nhk.or.jp/news/catnew.html',\n",
       "   'selector': '#main > article > section > div > ul > li:nth-child(1) > dl > dd > a > em',\n",
       "   'get_title': 'フィギュア NHK杯 鍵山優真3年ぶり優勝 宇野昌磨2位【全結果】',\n",
       "   'get_title_before': '【随時更新】ハマス 人質24人を解放 戦闘休止後初',\n",
       "   'update': True}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# スクレイピングの結果から、アップデートがあったものだけ配列に追加\n",
    "updates = []\n",
    "for web in websites:\n",
    "    if web[\"update\"] == True:\n",
    "        updates.append(web)\n",
    "\n",
    "# 一つ以上アップデートがある場合、ファイルを上書きして保存\n",
    "if 0 < len(updates):\n",
    "    print(\"get updates\")\n",
    "    japan_tz = pytz.timezone('Asia/Tokyo')\n",
    "    current_time_japan = datetime.now(japan_tz)\n",
    "    formatted_time = current_time_japan.strftime('%Y/%m/%d %H:%M')\n",
    "\n",
    "    new_object = {\n",
    "        \"date\": formatted_time,\n",
    "        \"updates\": updates\n",
    "    }\n",
    "\n",
    "    # 現状を読み込んで変数に格納\n",
    "    with open (\"update.json\", \"r\") as update:\n",
    "        update_json = json.load(update)\n",
    "        update_json.append(new_object)\n",
    "\n",
    "    # 更新分のオブジェクトを配列を追加して、jsonファイルとして保存\n",
    "    with open (\"update.json\", \"w\") as update:\n",
    "        json.dump(update_json, update, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # フロントエンド用にも保存する\n",
    "    with open (\"../docs/data/update.json\", \"w\") as update:\n",
    "        json.dump(update_json, update, ensure_ascii=False, indent=4)\n",
    "        \n",
    "else:\n",
    "    print(\"no update\")\n",
    "\n",
    "update_json[len(update_json)-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
